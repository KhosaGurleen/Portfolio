<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="project1 (part 1).css">
    <title>Project 1: Business Licence (Part 1)</title>
    <style>

      @media only screen and (max-width: 455px){
          body{

              background-color: black;
          }
          .boxes{
                flex-direction: column;
            }
      }
      
      /* @media not|only mediatype and (expressions) {
          CSS-Code;
      } */
            
      .boxes{
          display: flex;
      }

  </style>
</head>
<body>
  <div class="boxes, ">
    <h1>Project 1: Business Licence (Part 1)</h1>

    <div class="navbar">
      <a href="index.html">Home</a>
        <div class="dropdown">
          <button class="dropbtn">All Projects
            <i class="fa fa-caret-down"></i>
          </button>
          <div class="dropdown-content">
            <a href="Project 1 (Part 1).html">Project 1 (Part 1)</a>
            <a href="Project 1 (Part 2).html">Project 1 (Part 2)</a>
            <a href="Project 2 (UCW Domain).html">Project 2 (UCW Domain)</a>
          </div>
        </div>
        <a href="certifications.html">Certifications</a>
        <a href="badges.html">Badges</a>
      </div>

<div class="div">
  <h2>&nbsp; &nbsp;Descriptive Analysis</h2>
&nbsp; &nbsp;<b>Project Description:</b> Establishing AWS DAP for The City of Vancouver(Descriptive Analysis of Business licences issued per year)<br><br>
&nbsp; &nbsp;<b>Project Title:</b> DAP Design and Implementation for City of Vancouver(Business Licences in Downtown Vancouver)<br><br>
&nbsp; &nbsp;<b>Objective:</b> The City of Vancouver is in an advantageous place where the proper use of data can further improve decision-making, public services, and usage of resources. The establishment of the Data Analytic Platform (DAP) is essential for the city because it provides a foundation for the integration, processing, and visualization of a massive amount of data received from different sources. Through this platform, city officials can make the right decisions for the city's betterment by considering the analyzed data on the people’s quality of living, improved service delivery, and enhanced city planning.<br><br>
&nbsp; &nbsp;<b>Dataset:</b>Business licences data retrieved from city of Vancouver (open data portal) for year 2024 and 2023. The dataset includes the following key features:<br><br>
<div>&nbsp; &nbsp;RSN, Licence Number, Licence Revision Number, Business Name, Business Trade Name, Status, Issued Date	Expired Date, Business Type, Business SubType, Unit, Unit Type, House, Street, City, Province, Country, Postal Code, Local Area, Number of Employees, Fee Paid, Extract Date</div><br>



  <img class="img1" src="(DAP Part 1) dataset.jpg" alt="Sorry for the Inconvenience">
<br><br>

&nbsp; &nbsp;<b>Methodology:</b><br><br>
&nbsp; &nbsp;To be able to implement and migrate a data analytic platform of The City of Vancouver to the AWS, we have designed and implemented based on the 13 steps below:
<ol class="ol">
  <li><b>Step 1: Data Analytic Question Formulation:</b><br><br>
This step shows a breakdown of the data analysis process to improve business licence issuance and compliance in Downtown Vancouver. It starts with a high-level goal and breaks it down into different steps.
        <ul  class="ul"><br>
          <li>Descriptive Metric: The first step is to understand the current situation. This involves analyzing data on the number of business licenses issued per year. How many business licenses are issued per year?</li>
          <li>Diagnostic Metric: The next step is to identify the reasons for business license delays. This can be done by analyzing data about the reasons for the delay. Why are there delays in issuing licenses?</li>
          <li>Predictive Metric: The goal here is to predict the likelihood of a business license renewal. This can be done by analyzing data about past renewal patterns and other factors that might influence renewals. How likely is a business license to be renewed?</li>
          <li>Prescriptive Metric: Finally, the goal is to recommend proactive actions to improve renewal reminders. This involves analyzing data about successful reminders and developing new ways to ensure businesses are reminded about their licence renewal. How can we remind businesses about license renewals?</li>
        </ul>
    </li><br>
    <img class="img1" src="(DAP Part 1) step1.jpg" alt="Sorry for the Inconvenience"><br>
  <li ><b>Step 2: Data Discovery</b><br><br>
    Data discovery is about finding, understanding, and organizing data to set the stage for deeper analysis and informed decision-making. It involves data collection which means gathering data from various sources, such as databases, files, or external sources and then analyzing the collected data to understand its structure, quality, and content.<br><br>
    <img class="img1" src="(DAP Part 1) step2.jpg" alt="Sorry for the Inconvenience">
  </li><br>
  <li><b>Step 3: Data Storage Design</b><br><br>
    In this step, we need to store our operational data in an analytical environment using S3 service in AWS. To do this, firstly, we need to create a bucket for our data storage in S3. Bucket is a place where we store objects and object could be anything be it excel file, pdf, image, json file etc.
          <ul class="ul"><br>
            <li>Bucket Created: business-businesslicences-gurleen</li>
            <li>Organize by year: Inside the bucket, created folders for each year ("2023/Landing," "2024/Landing")</li>
            <li>Created folders for specific data: Within each year folder, made more folders for different types of data, like “business licences application records”, “business licences delay logs”, “business licences renewal history”, “business licences compliance data”, “business licences renewal reminder logs”, “process efficiency metrics” inside the “landing folder”</li>
          </ul><br>
          Finally, uploaded the dataset files into those relevant folders. This helps keep data organized and easy to find later.
          <br><br>
          <table>
            <tr>
              <td><img class="img2" src="(DAP Part 1) step3.jpg" alt="Sorry for the Inconvenience"></td>
              <td><img class="img2" src="(DAP Part 1) step3(1).jpg" alt="Sorry for the Inconvenience"></td>
            </tr>
            <tr>
              <td><img class="img2" src="(DAP Part 1) step3(2).jpg" alt="Sorry for the Inconvenience"></td>
              <td><img class="img2" src="(DAP Part 1) step3(3).jpg" alt="Sorry for the Inconvenience"></td>
            </tr>
        </table>
  </li><br>
  <li><b>Step 4: Data Preparation</b><br><br>
    Data Preparation is basically preparing or arranging your data from various sources to achieve your business goal which in my case was Business licences data retrieved from city of Vancouver (open data portal) for year 2024 and 2023.
    <table>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step4.jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step4(1).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
      <tr>
        <td colspan="2"><img class="img3" src="(DAP Part 1) step4(2).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
  </table><br><br>
  </li>
  <li><b>Step 5: Data Ingestion</b><br><br>
    Data Ingestion involves uploading your data inside your AWS bucket in specific folders. As in my case, uploading my excel and pdf files into their relevant folders. For instance, business licences application records.xlsx for Business Licences Application Records folder of my business bucket and so on.<br><br>
    <table>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step5.jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step5 (1).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
    </table><br>
  </li>
  <li><b>Step 6: Data Storage</b><br><br>
    Data storage is an essential step in data analytics platforms. It involves storing the collected data into folders of S3 buckets for efficient access and analysis. After gathering data from open data portal, it is then stored to the landing environment of S3.<br><br>
    <img class="img1" src="(DAP Part 1) step6.jpg" alt="Sorry for the Inconvenience">
  </li>
  <li><b>Step 7: Data Pipeline Design</b><br><br>
    The Data Pipeline design step is about planning how data will travel through the system, from where it starts to where it ends up. This includes deciding how data will be collected, processed, changed, and stored. The aim is to make sure data moves smoothly and is ready for analysis. I created visual representation of my ETL pipeline in draw.io using tables showing various stages of my design such as removing, filtering, extracting and grouping stage and then finally, reaching to my final outcome table showing “Licence Issuance Rate” calculations for year 2024 and 2023.<br><br>
    <table>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step7.jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step7(1).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step7(2).jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step7(3).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
      <tr></tr>
        <td colspan="2"><img class="img3" src="(DAP Part 1) step7(4).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
  </table><br><br>
  </li>
  <li><b>Step 8: Data Cleaning</b><br><br>
    In this step cleaned up the dataset of business licences application records in an AWS S3 bucket. The steps involve creating raw folder inside landing zone and and then using a data cleaning tool called AWS Glue DataBrew, I created a project. The project then cleaned the data by removing invalid, null or missing values using function feature inside it depending upon the percentage of missing values in the column, which will ensure the accuracy of any future analysis performed on the data. If had more than 80% missing values, I dropped that column because it was of no use.<br><br>
    <table>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step8.jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step8 (1).jpg" alt=""></td>
      </tr>
    </table><br><br>
  </li>
  <li><b>Step 9: Data Structuring</b><br><br>
    In this step, I arranged my data in structured manner meaning renamed columns names with relevant names with respect to the information they hold in them. Furthermore, I configured “Schema” in AWS Glue DataBrew to ensure whether my dataset had relevant datatypes for specific columns and if not changes them accordingly. Furthermore, I created and ran my job and stored the result of my cleaning and structuring inside the raw folder of my S3 bucket.<br><br>
    <table>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step9.jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step9 (1).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step9 (2).jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step9 (3).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
      <tr></tr>
        <td colspan="2"><img class="img3" src="(DAP Part 1) step9 (4).jpg" alt=""></td>
      </tr>
    </table><br><br>
  </li>
  <li><b>Step 10: Data Pipeline Implementation</b><br><br>
    This step involves creation of visual ETL using AWS Glue service. This step provides us with the summarized information for our analysis. In this step, I fetched data from my raw folder (cleaned and structured data) and then performed certain operations to that dataset to extract specific information. I used aggregation, filter and change schema to retrieve specific information from my dataset. Then I used to join function to group my dataset and performed average calculation on my dataset to get “Licence Issuance Rate” for year 2024 and 2023 using columns “Number of business licence issued per year” and “Total number of business licence applications initiated”.
    Licence Issuance Rate = (Number of business licence issued per year/ Total number of business licence applications initiated) *100
    Finally, I run my job, and my results were stored in the curated folder of my S3 bucket.<br><br>
    <table>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step10.jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step10 (1).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step10 (2).jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step10 (3).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step10 (4).jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step10 (5).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
      <tr>
        <td colspan="2"><img class="img3" src="(DAP Part 1) step10 (6).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
    </table><br><br>
  </li>
  <li><b>Step 11: Data Analysis</b><br><br>
    The AWS service used for executing this step is Amazon Athena. This step involved analyzing the summarized curated folder data from S3 bucket by creating tables for specific CSV files. The table contained columns such as Year and LIR (Licence Issuance Rate) for the years 2023 and 2024. After table creation, I ran SQL queries to retrieve specific information from the table using SQL “ORDER BY”, “SELECT” and various other queries.<br><br>
    <table>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step11.jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step11 (1).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step11 (2).jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step11 (3).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
    </table><br><br>
  </li>
  <li><b>Step 12: Data Visualization</b><br><br>
    In this step, I created visualizations for my Athena downloaded data file for Licence Issuance Rate containing 2024 and 2023 data. I made my visualizations for this step in excel using recommended charts and then downloaded that file in pdf format and renamed it as Graph_Report.<br><br>
    <table>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step12.jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step12 (1).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
    </table><br><br>
  </li>
  <li><b>Step 13: Data Publishing</b><br><br>
    AWS EC2 service was used to execute this step. This step involved publishing your data files to general and web servers to be accessible by the public. To do this step, I created two EC2 instances, one for general server and another one for web server. For connecting my instances, I used “Remote desktop connection” inbuild software in windows and uploaded my files to remote computer in analysis folder in C-Drive for general server and in wwwroot folder for web server and then using public IP address, I accessed my uploaded files on the web browser.<br><br>
    <table>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step13.jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step13 (1).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
      <tr>
        <td><img class="img2" src="(DAP Part 1) step13 (2).jpg" alt="Sorry for the Inconvenience"></td>
        <td><img class="img2" src="(DAP Part 1) step13 (3).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
      <tr></tr>
        <td colspan="2"><img class="img3" src="(DAP Part 1) step13 (4).jpg" alt="Sorry for the Inconvenience"></td>
      </tr>
    </table><br><br>
  </li>
  <li><b>DAP Estimated Cost</b><br><br>
    To estimate the cost of the dataset preparation phase for the above-listed projects using the AWS pricing calculator, we broke down the services used and their respective costs. The services in use include Amazon S3, AWS Glue, Amazon Athena, and Amazon EC2.<br>
    <ul>
      <li>Amazon Simple Storage Service (S3)
        <ul>
          <li>Storage Used: 527.9 GB per month</li>
          <li>Cost Estimate: 12.14 USD per month</li>
        </ul>
      </li>
      <li>AWS Glue
        <ul>
          <li>DPUs Used: Number of DPUs for Apache Spark job (10) and Number of DPUs for Python Shell job (5)</li>
          <li>Cost Estimate: 0.14 USD per month</li>
        </ul>
      </li>
      <li>Amazon Athena
        <ul>
          <li>Queries and Data Scanned: Total number of queries (7 per month), Amount of data scanned per query (varies)</li>
          <li>Cost Estimate: 9.99 USD per month</li>
        </ul>
      </li>
      <li>Amazon EC2
        <ul>
          <li>Instance Types: t2.micro</li>
          <li>Cost Estimate: 12.21 USD per month</li>
          </ul>
      </li>
      <li>Total Estimated Cost for Dataset Preparation Phase:
        <ul>
          <li>Monthly Cost: 17.54 USD</li>
          <li>Total 12 Months Cost: 210.48 USD</li>
        </ul>
      </li>
    </ul>
<br><br>
    <img class="img1" src="(DAP Part 1) cost.jpg" alt="Sorry for the Inconvenience">
  
  </li>
</ol>
&nbsp; &nbsp;This descriptive analysis project aims analyzing data on the number of business licenses issued per year.<br><br>
</div>
</div>
</body>
</html>